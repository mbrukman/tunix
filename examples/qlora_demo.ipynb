{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-gq1gcpyCpT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google/tunix/blob/main/examples/qlora_demo.ipynb\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTlz7JP7yCpT"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kagglehub\n",
        "\n",
        "# !pip install -q tensorflow\n",
        "# !pip install -q tensorboardX\n",
        "# !pip install -q grain\n",
        "# !pip install -q datasets\n",
        "# !pip install -q git+https://github.com/google/tunix\n",
        "# !pip install -q git+https://github.com/google/qwix\n",
        "\n",
        "# !pip uninstall -q -y flax\n",
        "# !pip install -q git+https://github.com/google/flax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjoS0-JyCpT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LshEMmSzx6W6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import time\n",
        "\n",
        "from flax import nnx\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import kagglehub\n",
        "import optax\n",
        "from orbax import checkpoint as ocp\n",
        "from qwix import lora\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma import data as data_lib\n",
        "from tunix.models.gemma import gemma as gemma_lib\n",
        "from tunix.models.gemma import params as params_lib\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.sft import peft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnEZ_jXwypn-"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QBcMaL22T3Uu"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Model\n",
        "MESH = [(1, 8), (\"fsdp\", \"tp\")]\n",
        "# LoRA\n",
        "RANK = 16\n",
        "ALPHA = 2.0\n",
        "\n",
        "# Train\n",
        "MAX_STEPS = 100\n",
        "EVAL_EVERY_N_STEPS = 20\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "\n",
        "# Checkpoint saving\n",
        "INTERMEDIATE_CKPT_DIR = \"/content/intermediate_ckpt/\"\n",
        "CKPT_DIR = \"/content/ckpts/\"\n",
        "PROFILING_DIR = \"/content/profiling/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3s5Qg6xT3Uu"
      },
      "source": [
        "## Load Gemma 2B\n",
        "\n",
        "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
        "to have agreed to the Gemma license\n",
        "[here](https://www.kaggle.com/models/google/gemma/flax/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o_7Sk8d7T3Uu"
      },
      "outputs": [],
      "source": [
        "# # Log in\n",
        "# if \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n",
        "#   kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oJC3Hfh9T3Uv"
      },
      "outputs": [],
      "source": [
        "# kaggle_ckpt_path = kagglehub.model_download(\"google/gemma/flax/2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r_5FWrq-T3Uv"
      },
      "outputs": [],
      "source": [
        "# # This is a workaround. The checkpoints on Kaggle don't work with NNX. So, we\n",
        "# # load the model, save the checkpoint locally, and then reload the model\n",
        "# # (sharded).\n",
        "# params = params_lib.load_and_format_params(os.path.join(kaggle_ckpt_path, \"2b\"))\n",
        "# gemma = gemma_lib.Transformer.from_params(params, version=\"2b\")\n",
        "# checkpointer = ocp.StandardCheckpointer()\n",
        "# _, state = nnx.split(gemma)\n",
        "# checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_ertAr6FT3Uv"
      },
      "outputs": [],
      "source": [
        "# # Wait for the ckpt to save successfully.\n",
        "# time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nGYKuLyFT3Uv"
      },
      "outputs": [],
      "source": [
        "# # Delete the intermediate model to save memory.\n",
        "# del params\n",
        "# del gemma\n",
        "# del state\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-30 21:39:24.784751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751319564.803841  385733 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751319564.809381  385733 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1751319564.824160  385733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751319564.824178  385733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751319564.824179  385733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1751319564.824181  385733 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# add the parent directory (one level up) to sys.path\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../../maxtext')))\n",
        "\n",
        "# ! pip install -r ../../maxtext/requirements.txt\n",
        "\n",
        "import MaxText as mt\n",
        "from MaxText import pyconfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ref_maxtext_model():\n",
        "\n",
        "  #python3 -m MaxText.train MaxText/configs/base.yml base_output_directory=${BASE_OUTPUT_DIRECTORY} dataset_path=${DATASET_PATH} tokenizer_path=assets/tokenizer.gemma load_parameters_path=${CONVERTED_CHECKPOINT} per_device_batch_size=1 run_name=${FINETUNE_RUN_NAME} max_target_length=8192 steps=10 async_checkpointing=false model_name=gemma-2b checkpoint_period=5\n",
        "\n",
        "  #TODO: @mazumdera: change this to use Gemma2-2b-it\n",
        "  config = pyconfig.initialize(\n",
        "      [\"\", \"../../maxtext/MaxText/configs/base.yml\"], #TODO: @mazumdera: why decode.py?\n",
        "      base_output_directory=\"gs://dummy_output_dir\",  # This is not used in Tunix.\n",
        "      run_name=\"test-tunix-maxtext-gemma-2b\",\n",
        "      # dataset_path=we use Tunix's dataset\n",
        "      load_parameters_path=\"gs://maxtext-gemma/2b/\",\n",
        "      tokenizer_path=\"../../maxtext/assets/tokenizer.gemma\",\n",
        "      per_device_batch_size=1,\n",
        "      max_target_length=8192,\n",
        "      steps=10,\n",
        "      async_checkpointing=\"false\",\n",
        "      model_name=\"gemma-2b\",\n",
        "      checkpoint_period=5,\n",
        "      skip_jax_distributed_system=\"true\"\n",
        "\n",
        "  )\n",
        "  model = mt.from_pretrained(config)\n",
        "  mesh  = model.mesh\n",
        "  \n",
        "  # We can continue to use Tunix's model_config\n",
        "  model_config = gemma_lib.TransformerConfig.gemma2_2b()\n",
        "  \n",
        "  return model, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HFKUZT10T3Uv"
      },
      "outputs": [],
      "source": [
        "def get_base_model(ckpt_path):\n",
        "\n",
        "  model_config = gemma_lib.TransformerConfig.gemma_2b()\n",
        "  mesh = jax.make_mesh(*MESH)\n",
        "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
        "      lambda: gemma_lib.Transformer(model_config, rngs=nnx.Rngs(params=0))\n",
        "  )\n",
        "  abs_state = nnx.state(abs_gemma)\n",
        "  abs_state = jax.tree.map(\n",
        "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "      abs_state,\n",
        "      nnx.get_named_sharding(abs_state, mesh),\n",
        "  )\n",
        "  checkpointer = ocp.StandardCheckpointer()\n",
        "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
        "\n",
        "  graph_def, _ = nnx.split(abs_gemma)\n",
        "  gemma = nnx.merge(graph_def, restored_params)\n",
        "  return gemma, mesh, model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KCPPEEi3T3Uv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating keys from env and command line: ['run_name', 'model_name', 'load_parameters_path', 'async_checkpointing', 'checkpoint_period', 'base_output_directory', 'tokenizer_path', 'per_device_batch_size', 'steps', 'skip_jax_distributed_system', 'max_target_length']\n",
            "Running Model: gemma-2b\n",
            "Updating following parameters in config\n",
            "\n",
            "base_emb_dim: 2048\n",
            "base_num_query_heads: 8\n",
            "base_num_kv_heads: 1\n",
            "base_mlp_dim: 16384\n",
            "base_num_decoder_layers: 18\n",
            "head_dim: 256\n",
            "mlp_activations: ['gelu', 'linear']\n",
            "vocab_size: 256128\n",
            "decoder_block: gemma\n",
            "normalization_layer_epsilon: 1e-06\n",
            "logits_via_embedding: True\n",
            "Updating keys from model: ['base_emb_dim', 'base_num_query_heads', 'base_num_kv_heads', 'base_mlp_dim', 'base_num_decoder_layers', 'head_dim', 'mlp_activations', 'vocab_size', 'decoder_block', 'normalization_layer_epsilon', 'logits_via_embedding']\n",
            "Skipping jax distributed system due to skip_jax_distributed_system=True flag.\n",
            "Not using emergency checkpoint, ignoring local_checkpoint_directory, local_checkpoint_period, use_replicator_service and replicator_backup_interval_minutes\n",
            "dataset_type set to tfds, will use keys['dataset_path']='' and keys['dataset_name']='c4/en:3.0.1'\n",
            "Config param activations_in_float32: False\n",
            "Config param adam_b1: 0.9\n",
            "Config param adam_b2: 0.95\n",
            "Config param adam_eps: 1e-08\n",
            "Config param adam_eps_root: 0.0\n",
            "Config param adam_weight_decay: 0.1\n",
            "Config param add_bos: True\n",
            "Config param add_eos: True\n",
            "Config param allow_split_physical_axes: False\n",
            "Config param ar_cache_axis_order: 1,2,0,3\n",
            "Config param async_checkpointing: False\n",
            "Config param attention: autoselected\n",
            "Config param attention_type: global\n",
            "Config param attn_logits_soft_cap: None\n",
            "Config param autoregressive_decode_assert: \n",
            "Config param base_emb_dim: 2048\n",
            "Config param base_mlp_dim: 16384\n",
            "Config param base_moe_mlp_dim: 7168\n",
            "Config param base_num_decoder_layers: 18\n",
            "Config param base_num_kv_heads: 1\n",
            "Config param base_num_query_heads: 8\n",
            "Config param base_output_directory: gs://dummy_output_dir\n",
            "Config param beta_fast: 32\n",
            "Config param beta_slow: 1\n",
            "Config param capacity_factor: -1.0\n",
            "Config param cast_logits_to_fp32: True\n",
            "Config param checkpoint_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/checkpoints/\n",
            "Config param checkpoint_is_quantized: False\n",
            "Config param checkpoint_period: 5\n",
            "Config param checkpoint_storage_concurrent_gb: 96\n",
            "Config param checkpoint_storage_target_data_file_size_bytes: 2147483648\n",
            "Config param checkpoint_storage_use_ocdbt: True\n",
            "Config param checkpoint_storage_use_zarr3: True\n",
            "Config param chunk_attn_window_size: 0\n",
            "Config param collect_stack_trace: False\n",
            "Config param colocated_python_data_input: False\n",
            "Config param compile_topology: \n",
            "Config param compile_topology_num_slices: -1\n",
            "Config param compiled_trainstep_file: \n",
            "Config param compute_axis_order: 0,1,2,3\n",
            "Config param context: remat\n",
            "Config param context_parallel_load_balance: True\n",
            "Config param cosine_learning_rate_final_fraction: 0.1\n",
            "Config param custom_mesh: \n",
            "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive'),)\n",
            "Config param data_shuffle_seed: 0\n",
            "Config param dataset_name: c4/en:3.0.1\n",
            "Config param dataset_path: \n",
            "Config param dataset_type: tfds\n",
            "Config param dcn_autoregressive_parallelism: 1\n",
            "Config param dcn_context_autoregressive_parallelism: 1\n",
            "Config param dcn_context_parallelism: 1\n",
            "Config param dcn_data_parallelism: -1\n",
            "Config param dcn_expert_parallelism: 1\n",
            "Config param dcn_fsdp_parallelism: 1\n",
            "Config param dcn_fsdp_transpose_parallelism: 1\n",
            "Config param dcn_parallelism: [-1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param dcn_pipeline_parallelism: 1\n",
            "Config param dcn_sequence_parallelism: 1\n",
            "Config param dcn_tensor_parallelism: 1\n",
            "Config param dcn_tensor_sequence_parallelism: 1\n",
            "Config param dcn_tensor_transpose_parallelism: 1\n",
            "Config param decode_sampling_nucleus_p: -1\n",
            "Config param decode_sampling_strategy: greedy\n",
            "Config param decode_sampling_temperature: 1.0\n",
            "Config param decode_sampling_top_k: 0\n",
            "Config param decoder_block: DecoderBlockType.GEMMA\n",
            "Config param decoder_layer_input: device\n",
            "Config param dpo_beta: 0.1\n",
            "Config param dpo_label_smoothing: 0.0\n",
            "Config param dropout_rate: 0.0\n",
            "Config param dtype: bfloat16\n",
            "Config param dtype_mm: float32\n",
            "Config param dump_hlo: False\n",
            "Config param dump_hlo_delete_local_after: True\n",
            "Config param dump_hlo_gcs_dir: \n",
            "Config param dump_hlo_local_dir: /tmp/xla_dump/\n",
            "Config param dump_hlo_module_name: jit_train_step\n",
            "Config param dump_hlo_upload_all: False\n",
            "Config param dump_hlo_xla_flags: \n",
            "Config param dump_step: -1\n",
            "Config param emb_dim: 2048\n",
            "Config param enable_checkpoint_cloud_logger: False\n",
            "Config param enable_checkpointing: True\n",
            "Config param enable_data_shuffling: True\n",
            "Config param enable_dropout: True\n",
            "Config param enable_emergency_checkpoint: False\n",
            "Config param enable_gcp_goodput_metrics: True\n",
            "Config param enable_gcp_step_deviation_metrics: True\n",
            "Config param enable_goodput_recording: False\n",
            "Config param enable_jax_profiler: False\n",
            "Config param enable_llm_inference_pool: False\n",
            "Config param enable_model_warmup: False\n",
            "Config param enable_padding_causal_mask: True\n",
            "Config param enable_pathways_goodput: False\n",
            "Config param enable_prefix_caching: False\n",
            "Config param enable_single_controller: False\n",
            "Config param enable_single_replica_ckpt_restoring: False\n",
            "Config param enable_tensorboard: True\n",
            "Config param eval_data_columns: ['text']\n",
            "Config param eval_dataset_name: c4/en:3.0.1\n",
            "Config param eval_interval: -1\n",
            "Config param eval_per_device_batch_size: 1.0\n",
            "Config param eval_split: validation\n",
            "Config param eval_steps: -1\n",
            "Config param expansion_factor_real_data: -1\n",
            "Config param final_logits_soft_cap: None\n",
            "Config param first_num_dense_layers: 0\n",
            "Config param float32_logits: False\n",
            "Config param float32_qk_product: False\n",
            "Config param force_unroll: False\n",
            "Config param freeze_vision_encoder_params: True\n",
            "Config param fused_mlp: False\n",
            "Config param fused_qkv: False\n",
            "Config param gcs_metrics: False\n",
            "Config param generate_slice: v5e-16\n",
            "Config param global_batch_size_to_eval_on: 1\n",
            "Config param global_batch_size_to_load: 1\n",
            "Config param global_batch_size_to_load_eval: 1\n",
            "Config param global_batch_size_to_train_on: 1\n",
            "Config param global_parameter_scale: 1\n",
            "Config param goodput_upload_interval_seconds: 30\n",
            "Config param gradient_accumulation_steps: 1\n",
            "Config param gradient_clipping_threshold: 1.0\n",
            "Config param grain_eval_files: \n",
            "Config param grain_file_type: arrayrecord\n",
            "Config param grain_train_files: \n",
            "Config param grain_worker_count: 1\n",
            "Config param grain_worker_count_eval: 1\n",
            "Config param hardware: tpu\n",
            "Config param head_dim: 256\n",
            "Config param heartbeat_reporting_interval_in_seconds: 5\n",
            "Config param hf_data_dir: \n",
            "Config param hf_eval_files: \n",
            "Config param hf_eval_split: \n",
            "Config param hf_path: \n",
            "Config param hf_train_files: \n",
            "Config param hidden_size_for_vit: 1408\n",
            "Config param ici_autoregressive_parallelism: 1\n",
            "Config param ici_context_autoregressive_parallelism: 1\n",
            "Config param ici_context_parallelism: 1\n",
            "Config param ici_data_parallelism: 1\n",
            "Config param ici_expert_parallelism: 1\n",
            "Config param ici_fsdp_parallelism: -1\n",
            "Config param ici_fsdp_transpose_parallelism: 1\n",
            "Config param ici_parallelism: [1, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Config param ici_pipeline_parallelism: 1\n",
            "Config param ici_sequence_parallelism: 1\n",
            "Config param ici_tensor_parallelism: 1\n",
            "Config param ici_tensor_sequence_parallelism: 1\n",
            "Config param ici_tensor_transpose_parallelism: 1\n",
            "Config param image_path: \n",
            "Config param image_size_for_vit: 896\n",
            "Config param inference_benchmark_test: False\n",
            "Config param inference_metadata_file: \n",
            "Config param inference_microbenchmark_log_file_path: \n",
            "Config param inference_microbenchmark_loop_iters: 10\n",
            "Config param inference_microbenchmark_num_samples: [1, 2, 3, 4, 5]\n",
            "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
            "Config param inference_microbenchmark_stages: prefill,generate\n",
            "Config param inference_server: MaxtextInterleavedServer\n",
            "Config param inhomogeneous_layer_cycle_interval: 1\n",
            "Config param init_weights_seed: 0\n",
            "Config param input_data_sharding_logical_axes: ['activation_embed_and_logits_batch', 'activation_norm_length']\n",
            "Config param interleave_moe_layer_step: 1\n",
            "Config param intermediate_size_for_vit: 5632\n",
            "Config param jax_cache_dir: ~/jax_cache\n",
            "Config param jax_debug_log_modules: \n",
            "Config param jax_distributed_initialization_timeout: 300\n",
            "Config param jax_profiler_port: 9999\n",
            "Config param key_proj: remat\n",
            "Config param kv_lora_rank: 512\n",
            "Config param kv_quant_axis: heads_and_dkv\n",
            "Config param kv_quant_dtype: int8\n",
            "Config param learning_rate: 3e-05\n",
            "Config param learning_rate_schedule_steps: 10\n",
            "Config param load_balance_loss_weight: 0.01\n",
            "Config param load_from_prefill_dir: False\n",
            "Config param load_full_state_path: \n",
            "Config param load_parameters_path: gs://maxtext-gemma/2b/\n",
            "Config param local_checkpoint_directory: \n",
            "Config param local_checkpoint_period: 0\n",
            "Config param local_rope_max_timescale: -1\n",
            "Config param log_config: True\n",
            "Config param log_period: 100\n",
            "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence', 'autoregressive')), ('activation_kv_heads', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_length', ('sequence', 'context')), ('activation_length', ('context',)), ('activation_norm_length', ('tensor_sequence', 'context', 'sequence')), ('activation_q_length', ('context',)), ('activation_kv_length', ()), ('activation_embed', ('tensor', 'tensor_transpose')), ('activation_mlp', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_kv', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_prefill_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose', 'sequence', 'tensor_sequence')), ('activation_vocab', ('tensor', 'tensor_transpose')), ('activation_vocab', 'tensor_sequence'), ('activation_vocab', ('sequence', 'context')), ('activation_stage', 'stage'), ('activation_exp', ('expert',)), ('decode_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('decode_length', ('sequence',)), ('mlp', ('fsdp_transpose', 'tensor', 'tensor_sequence', 'autoregressive')), ('vocab', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('q_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('kv_heads', ('tensor', 'tensor_transpose', 'tensor_sequence', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'tensor_transpose', 'context', 'expert')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('embed', ('fsdp', 'sequence', 'context', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'tensor_transpose', 'context')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence', 'context')), ('embed_no_exp', ('fsdp', 'sequence', 'context')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('q_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('q_lora', ('fsdp', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'tensor_transpose', 'expert')), ('kv_lora', ('fsdp', 'fsdp_transpose', 'sequence', 'context', 'expert')), ('kv_lora', ('fsdp', 'sequence', 'context', 'expert')), ('norm', ('tensor', 'tensor_transpose', 'tensor_sequence')), ('layers', 'stage'), ('kv', ()), ('kv_head_dim', ()), ('cache_batch_prefill', ()), ('cache_batch', ()), ('cache_heads_none', ()), ('cache_heads', ('autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence')), ('cache_heads', ('autoregressive', 'tensor', 'tensor_sequence')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'), ('paged_kv_heads', ('tensor',)), ('num_pages', ()), ('tokens_per_page', ()), ('paged_kv_head_dim_size', ()))\n",
            "Config param logits_dot_in_fp32: False\n",
            "Config param logits_via_embedding: True\n",
            "Config param lora_input_adapters_path: \n",
            "Config param matmul_precision: default\n",
            "Config param max_checkify: False\n",
            "Config param max_corpus_chars: 10000000\n",
            "Config param max_position_embeddings: 163840\n",
            "Config param max_prefill_predict_length: 64\n",
            "Config param max_target_length: 8192\n",
            "Config param megablox: True\n",
            "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'context', 'context_autoregressive', 'tensor', 'tensor_transpose', 'tensor_sequence', 'expert', 'autoregressive']\n",
            "Config param metrics_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/metrics/\n",
            "Config param metrics_file: \n",
            "Config param micro_batch_size_to_eval_on: 1\n",
            "Config param micro_batch_size_to_train_on: 1\n",
            "Config param mla_naive_kvcache: True\n",
            "Config param mlp_activations: ['gelu', 'linear']\n",
            "Config param mlp_dim: 16384\n",
            "Config param mlpwi: remat\n",
            "Config param mlpwi_0: remat\n",
            "Config param mlpwi_1: remat\n",
            "Config param mlpwo: remat\n",
            "Config param model_call_mode: \n",
            "Config param model_name: gemma-2b\n",
            "Config param moe_mlp_dim: 7168\n",
            "Config param monitor_goodput: False\n",
            "Config param monitor_step_time_deviation: True\n",
            "Config param mscale: 1.0\n",
            "Config param mu_dtype: float32\n",
            "Config param multi_sampling: False\n",
            "Config param n_routing_groups: -1\n",
            "Config param nope_layer_interval: -1\n",
            "Config param normalization_layer_epsilon: 1e-06\n",
            "Config param normalize_embedding_logits: True\n",
            "Config param num_attention_heads_for_vit: 16\n",
            "Config param num_channels_for_vit: 3\n",
            "Config param num_decoder_layers: 18\n",
            "Config param num_epoch: 1\n",
            "Config param num_experts: 1\n",
            "Config param num_experts_per_tok: 1\n",
            "Config param num_hidden_layers_for_vit: 34\n",
            "Config param num_kv_heads: 1\n",
            "Config param num_layers_per_pipeline_stage: 1\n",
            "Config param num_pipeline_microbatches: -1\n",
            "Config param num_pipeline_repeats: -1\n",
            "Config param num_query_heads: 8\n",
            "Config param num_slices: 1\n",
            "Config param opt_type: adamw\n",
            "Config param optimize_mesh_for_tpu_v6e: False\n",
            "Config param optimizer_memory_host_offload: False\n",
            "Config param original_max_position_embeddings: 4096\n",
            "Config param out_proj: remat\n",
            "Config param override_model_config: False\n",
            "Config param packing: True\n",
            "Config param pagedattn_head_dim_alignment: 128\n",
            "Config param pagedattn_max_pages_per_group: 256\n",
            "Config param pagedattn_num_pages: 64\n",
            "Config param pagedattn_pages_per_compute_block: 4\n",
            "Config param pagedattn_tokens_per_page: 32\n",
            "Config param param_scan_axis: 1\n",
            "Config param parameter_memory_host_offload: False\n",
            "Config param patch_size_for_vit: 14\n",
            "Config param per_device_batch_size: 1.0\n",
            "Config param pipeline_delay_activation_forwarding: False\n",
            "Config param pipeline_fsdp_ag_once: False\n",
            "Config param pipeline_parallel_layers: -1\n",
            "Config param pixel_shuffle_ratio_for_vit: 0.5\n",
            "Config param prefill_cache_axis_order: 1,2,0,3\n",
            "Config param prefill_cache_dir: \n",
            "Config param prefill_chunk_size: 256\n",
            "Config param prefill_slice: v5e-16\n",
            "Config param prefix_caching_dram_byte: 100000000000\n",
            "Config param prefix_caching_hbm_byte: 10000000000\n",
            "Config param profile_cleanly: True\n",
            "Config param profile_periodically_period: -1\n",
            "Config param profiler: \n",
            "Config param profiler_steps: 5\n",
            "Config param projector_dropout_for_vit: 0.0\n",
            "Config param projector_input_dim_for_vit: 4096\n",
            "Config param projector_output_dim_for_vit: 4096\n",
            "Config param prometheus_port: 0\n",
            "Config param prompt: I love to\n",
            "Config param q_lora_rank: 0\n",
            "Config param qk_nope_head_dim: 128\n",
            "Config param qk_rope_head_dim: 64\n",
            "Config param qkv_proj: remat\n",
            "Config param quant_cfg_path: \n",
            "Config param quantization: \n",
            "Config param quantization_local_shard_count: 1\n",
            "Config param quantize_kvcache: False\n",
            "Config param query_proj: remat\n",
            "Config param ragged_block_size: 256\n",
            "Config param record_internal_nn_metrics: 0\n",
            "Config param remat_policy: full\n",
            "Config param remat_policy_for_vit: minimal\n",
            "Config param replicate_quant_scale: False\n",
            "Config param replicator_backup_interval_minutes: 0\n",
            "Config param report_heartbeat_metric_for_gcp_monitoring: False\n",
            "Config param report_performance_metric_for_gcp_monitoring: False\n",
            "Config param reshape_q: False\n",
            "Config param return_log_prob: False\n",
            "Config param reuse_example_batch: 0\n",
            "Config param rope_factor: 40\n",
            "Config param rope_max_timescale: 10000\n",
            "Config param rope_min_timescale: 1\n",
            "Config param rope_theta_for_vit: 10000\n",
            "Config param rope_type: default\n",
            "Config param rope_use_scale: True\n",
            "Config param routed_bias: False\n",
            "Config param routed_scaling_factor: 1.0\n",
            "Config param routed_score_func: \n",
            "Config param run_name: test-tunix-maxtext-gemma-2b\n",
            "Config param sa_block_kv: 512\n",
            "Config param sa_block_kv_compute: 512\n",
            "Config param sa_block_kv_dkv: 512\n",
            "Config param sa_block_kv_dkv_compute: 512\n",
            "Config param sa_block_kv_dq: 512\n",
            "Config param sa_block_q: 512\n",
            "Config param sa_block_q_dkv: 512\n",
            "Config param sa_block_q_dq: 512\n",
            "Config param sa_k_layout: HEAD_DIM_MINOR\n",
            "Config param sa_q_layout: HEAD_DIM_MINOR\n",
            "Config param sa_use_fused_bwd_kernel: False\n",
            "Config param sa_v_layout: HEAD_DIM_MINOR\n",
            "Config param save_config_to_gcs: False\n",
            "Config param save_quantized_params_path: \n",
            "Config param scan_layers: True\n",
            "Config param scan_layers_per_stage: False\n",
            "Config param scan_pipeline_iterations: True\n",
            "Config param set_remat_policy_on_layers_per_stage: False\n",
            "Config param set_remat_policy_on_pipeline_iterations: True\n",
            "Config param sft_train_on_completion_only: False\n",
            "Config param sharding_tolerance: 0.02\n",
            "Config param shared_experts: 1\n",
            "Config param skip_first_n_steps_for_profiler: 1\n",
            "Config param skip_jax_distributed_system: True\n",
            "Config param sliding_window_size: 0\n",
            "Config param sparse_matmul: True\n",
            "Config param stack_prefill_result_cache: False\n",
            "Config param stack_trace_interval_seconds: 600\n",
            "Config param stack_trace_to_cloud: False\n",
            "Config param step_deviation_interval_seconds: 30\n",
            "Config param steps: 10\n",
            "Config param subslice_shape: \n",
            "Config param target_eval_loss: 0.0\n",
            "Config param temperature_tuning: False\n",
            "Config param tensorboard_dir: gs://dummy_output_dir/test-tunix-maxtext-gemma-2b/tensorboard/\n",
            "Config param tile_activation_dim: 1024\n",
            "Config param tile_batch_seq: 512\n",
            "Config param tile_size_for_vit: 336\n",
            "Config param tile_weight_dim: 1024\n",
            "Config param tokenize_eval_data: True\n",
            "Config param tokenize_train_data: True\n",
            "Config param tokenizer_path: ../../maxtext/assets/tokenizer.gemma\n",
            "Config param tokenizer_type: sentencepiece\n",
            "Config param topk_routing_group: -1\n",
            "Config param train_data_columns: ['text']\n",
            "Config param train_split: train\n",
            "Config param trainable_position_size: -1\n",
            "Config param upload_all_profiler_results: False\n",
            "Config param use_chat_template: False\n",
            "Config param use_chunked_prefill: False\n",
            "Config param use_dpo: False\n",
            "Config param use_iota_embed: False\n",
            "Config param use_multimodal: False\n",
            "Config param use_post_attn_norm: False\n",
            "Config param use_post_ffw_norm: False\n",
            "Config param use_qk_norm: False\n",
            "Config param use_ragged_attention: False\n",
            "Config param use_random_routing: False\n",
            "Config param use_replicator_service: False\n",
            "Config param use_sft: False\n",
            "Config param use_untrainable_positional_embedding: False\n",
            "Config param use_vertex_tensorboard: False\n",
            "Config param using_pipeline_parallelism: False\n",
            "Config param v_head_dim: 128\n",
            "Config param value_proj: remat\n",
            "Config param vertex_tensorboard_project: \n",
            "Config param vertex_tensorboard_region: \n",
            "Config param vision_output_dim_for_vit: 4096\n",
            "Config param vocab_size: 256128\n",
            "Config param warmup_steps_fraction: 0.1\n",
            "Config param weight_dtype: float32\n",
            "Num_devices: 1, shape (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<script> (()=>{ if (customElements.get('treescope-container') === undefined) { class TreescopeContainer extends HTMLElement { constructor() { super(); this.attachShadow({mode: \"open\"}); this.defns = {}; this.state = {}; } } customElements.define(\"treescope-container\", TreescopeContainer); } if (customElements.get('treescope-run-here') === undefined) { class RunHere extends HTMLElement { constructor() { super() } connectedCallback() { const run = child => { const fn = new Function(child.textContent); child.textContent = \"\"; fn.call(this); this.remove(); }; const child = this.querySelector(\"script\"); if (child) { run(child); } else { new MutationObserver(()=>{ run(this.querySelector(\"script\")); }).observe(this, {childList: true}); } } } customElements.define(\"treescope-run-here\", RunHere); } })(); </script> <treescope-container class=\"treescope_out_858a8c3a560c435abf66d8e6c047e43f\" style=\"display:block\"></treescope-container> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_858a8c3a560c435abf66d8e6c047e43f\")) .filter((elt) => !elt.dataset.setup) )[0]; root.dataset.setup = 1; const msg = document.createElement(\"span\"); msg.style = \"color: #cccccc; font-family: monospace;\"; msg.textContent = \"(Loading...)\"; root.state.loadingMsg = msg; root.shadowRoot.appendChild(msg); root.state.chain = new Promise((resolve, reject) => { const observer = new IntersectionObserver((entries) => { for (const entry of entries) { if (entry.isIntersecting) { resolve(); observer.disconnect(); return; } } }, {rootMargin: \"1000px\"}); window.setTimeout(() => { observer.observe(root); }, 0); }); root.state.deferring = false; const _insertNode = (node) => { for (let oldScript of node.querySelectorAll(\"script\")) { let newScript = document.createElement(\"script\"); newScript.type = oldScript.type; newScript.textContent = oldScript.textContent; oldScript.parentNode.replaceChild(newScript, oldScript); } if (root.state.loadingMsg) { root.state.loadingMsg.remove(); root.state.loadingMsg = null; } root.shadowRoot.appendChild(node); }; root.defns.insertContent = ((contentNode, compressed) => { if (compressed) { root.state.deferring = true; } if (root.state.deferring) { root.state.chain = (async () => { await root.state.chain; if (compressed) { const encoded = contentNode.textContent; const blob = new Blob([ Uint8Array.from(atob(encoded), (m) => m.codePointAt(0)) ]); const reader = blob.stream().pipeThrough( new DecompressionStream(\"deflate\") ).pipeThrough( new TextDecoderStream(\"utf-8\") ).getReader(); const parts = []; while (true) { const step = await reader.read(); if (step.done) { break; } parts.push(step.value); } const tpl = document.createElement('template'); tpl.innerHTML = parts.join(\"\"); _insertNode(tpl.content); } else { _insertNode(contentNode.content); } })(); } else { _insertNode(contentNode.content); } }); </script></treescope-run-here><div style=\"display:none\"> <script type=\"application/octet-stream\" >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</script> <treescope-run-here><script type=\"application/octet-stream\"> const root = ( Array.from(document.getElementsByClassName( \"treescope_out_858a8c3a560c435abf66d8e6c047e43f\")) .filter((elt) => !elt.dataset['step0']) )[0]; root.dataset['step0'] = 1; root.defns.insertContent( this.parentNode.querySelector('script[type=\"application/octet-stream\"]'), true ); this.parentNode.remove(); </script></treescope-run-here> </div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Base model\n",
        "# gemma, mesh, model_config = get_base_model(\n",
        "#     ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
        "# )\n",
        "\n",
        "gemma, mesh, model_config = get_ref_maxtext_model()\n",
        "gemma_maxtext_nnx = nnx.bridge.ToNNX(gemma)\n",
        "nnx.display(gemma_maxtext_nnx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2iDYbnsT3Uv"
      },
      "source": [
        "## Prompt the model\n",
        "\n",
        "Let's see how the model performs on the English-French translation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iH82cHpAT3Uv"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     17\u001b[39m sampler = sampler_lib.Sampler(\n\u001b[32m     18\u001b[39m     transformer=gemma_maxtext_nnx,\n\u001b[32m     19\u001b[39m     tokenizer=gemma_tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     ),\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m input_batch = [\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTranslate this into French:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHello, my name is Morgane.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTranslate this into French:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThis dish is delicious!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTranslate this into French:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mI am a student.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTranslate this into French:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mHow\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather today?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m out_data = \u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_strings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_generation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The number of steps performed when generating a response.\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_string, out_string \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_batch, out_data.text):\n\u001b[32m     41\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m----------------------\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:693\u001b[39m, in \u001b[36mSampler.__call__\u001b[39m\u001b[34m(self, input_strings, total_generation_steps, max_prompt_length, echo, return_logits, forbidden_tokens, temperature, top_p, top_k, beam_size, seed, pad_output)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    692\u001b[39m   seed = jax.random.PRNGKey(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m sampling_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_sample_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_sampling_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_sampling_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforbidden_token_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforbidden_token_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m._compiled_prefill_fn(\n\u001b[32m    705\u001b[39m     \u001b[38;5;28mself\u001b[39m._flattened_transformer_state, sampling_state\n\u001b[32m    706\u001b[39m )\n\u001b[32m    708\u001b[39m sampling_state = \u001b[38;5;28mself\u001b[39m._compiled_decode_fn(\n\u001b[32m    709\u001b[39m     \u001b[38;5;28mself\u001b[39m._flattened_transformer_state, sampling_state\n\u001b[32m    710\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:338\u001b[39m, in \u001b[36mSampler.init_sample_state\u001b[39m\u001b[34m(self, all_input_ids, total_sampling_steps, include_logits, forbidden_token_ids, temperature, top_p, top_k, seed, beam_size)\u001b[39m\n\u001b[32m    328\u001b[39m positions = utils.build_positions_from_mask(input_mask)\n\u001b[32m    330\u001b[39m done = jnp.zeros((batch_size,), dtype=jnp.bool_)\n\u001b[32m    332\u001b[39m cache = _init_cache(\n\u001b[32m    333\u001b[39m     n_layers=\u001b[38;5;28mself\u001b[39m.cache_config.num_layers,\n\u001b[32m    334\u001b[39m     cache_size=\u001b[38;5;28mself\u001b[39m.cache_config.cache_size,\n\u001b[32m    335\u001b[39m     batch_size=batch_size,\n\u001b[32m    336\u001b[39m     num_kv_heads=\u001b[38;5;28mself\u001b[39m.cache_config.num_kv_heads,\n\u001b[32m    337\u001b[39m     head_dim=\u001b[38;5;28mself\u001b[39m.cache_config.head_dim,\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     dtype=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m,\n\u001b[32m    339\u001b[39m )\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_logits:\n\u001b[32m    342\u001b[39m   logits_buffer = jnp.zeros(\n\u001b[32m    343\u001b[39m       (batch_size, buffer_size, \u001b[38;5;28mself\u001b[39m.transformer.num_embed),\n\u001b[32m    344\u001b[39m       dtype=jnp.float32,\n\u001b[32m    345\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/generate/sampler.py:296\u001b[39m, in \u001b[36mSampler.dtype\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> jnp.dtype:\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flattened_transformer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.dtype\n",
            "\u001b[31mIndexError\u001b[39m: list index out of range"
          ]
        }
      ],
      "source": [
        "# gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "#     os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        "# )\n",
        "\n",
        "from MaxText.input_pipeline import _input_pipeline_utils\n",
        "from MaxText.globals import PKG_DIR\n",
        "\n",
        "# gemma_tokenizer = _input_pipeline_utils.get_tokenizer(\n",
        "#         os.path.join(os.path.dirname(PKG_DIR), \"assets\", \"tokenizer_llama3.tiktoken\"),\n",
        "#         \"tiktoken\",\n",
        "#         add_bos=True,\n",
        "#         add_eos=False,\n",
        "#     )\n",
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=gemma_maxtext_nnx,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdxn7DQYT3Uv"
      },
      "source": [
        "## Apply LoRA/QLoRA to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3t-7leAT3Uv"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = lora.LoraProvider(\n",
        "      module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\",\n",
        "      rank=RANK,\n",
        "      alpha=ALPHA,\n",
        "      # comment the two args below for LoRA (w/o quantisation).\n",
        "      weight_qtype=\"nf4\",\n",
        "      tile_size=256,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = lora.apply_lora_to_model(\n",
        "      base_model, lora_provider, **model_input\n",
        "  )\n",
        "\n",
        "  with mesh:\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfmodoqrT3Uv"
      },
      "outputs": [],
      "source": [
        "# LoRA model\n",
        "lora_gemma = get_lora_model(gemma, mesh=mesh)\n",
        "nnx.display(lora_gemma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD6-iU0PT3Uv"
      },
      "source": [
        "## Load Datasets for SFT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6T_7Cik8T3Uv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Variant folder /home/mazumdera/tensorflow_datasets/mtnt/en-fr/1.0.0 has no dataset_info.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/mazumdera/tensorflow_datasets/mtnt/en-fr/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0435cd0c6a914692a484acd269b7bb2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "534c4408691f4f10a64ba97cc8d77361",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d687cf2165a04905a43b219f8ccdd58c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02e941db98574be6ac938b04fa2efd69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a48a3ec5a1fa4c728f1efdbfb4b05bdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67e2a58341dc4cfa8ed942a788462432",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /home/mazumdera/tensorflow_datasets/mtnt/en-fr/incomplete.E61XZZ_1.0.0/mtnt-train.array_record*...: "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1262b1eb5e9c47bfa3f76d8df017fa29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test examples...: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "137dce28364542a3a2e426479d2e3c57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /home/mazumdera/tensorflow_datasets/mtnt/en-fr/incomplete.E61XZZ_1.0.0/mtnt-test.array_record*...:  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af46191d60ee40e9b13a323b3561b6ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating valid examples...: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dca446eccad14cecbce1c4f19c06e510",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /home/mazumdera/tensorflow_datasets/mtnt/en-fr/incomplete.E61XZZ_1.0.0/mtnt-valid.array_record*...: "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset mtnt downloaded and prepared to /home/mazumdera/tensorflow_datasets/mtnt/en-fr/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Loads the training and validation datasets\n",
        "train_ds, validation_ds = data_lib.create_datasets(\n",
        "    dataset_name='mtnt/en-fr',\n",
        "    # Uncomment the line below to use a Hugging Face dataset.\n",
        "    # Note that this requires upgrading the 'datasets' package and restarting\n",
        "    # the Colab runtime.\n",
        "    # dataset_name='Helsinki-NLP/opus-100',\n",
        "    global_batch_size=BATCH_SIZE,\n",
        "    max_target_length=256,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    tokenizer=gemma_tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  pad_mask = x.input_tokens != gemma_tokenizer.pad_id()\n",
        "  positions = gemma_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = gemma_lib.make_causal_attn_mask(pad_mask)\n",
        "  return {\n",
        "      'input_tokens': x.input_tokens,\n",
        "      'input_mask': x.input_mask,\n",
        "      'positions': positions,\n",
        "      'attention_mask': attention_mask,\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w0PHlVBT3Uv"
      },
      "source": [
        "## SFT Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1xPieRT3Uv"
      },
      "source": [
        "### Training with full weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oYR9JKNT3Uv"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unsupported type: <class 'MaxText.layers.models.Transformer'>, this is a bug.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m logging_option = metrics_logger.MetricsLoggerOptions(\n\u001b[32m      2\u001b[39m     log_dir=\u001b[33m\"\u001b[39m\u001b[33m/tmp/tensorboard/full\u001b[39m\u001b[33m\"\u001b[39m, flush_every_n_steps=\u001b[32m20\u001b[39m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m training_config = peft_trainer.TrainingConfig(\n\u001b[32m      5\u001b[39m     eval_every_n_steps=EVAL_EVERY_N_STEPS,\n\u001b[32m      6\u001b[39m     max_steps=MAX_STEPS,\n\u001b[32m      7\u001b[39m     metrics_logging_options=logging_option,\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m trainer = \u001b[43mpeft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPeftTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptax\u001b[49m\u001b[43m.\u001b[49m\u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m jax.profiler.trace(os.path.join(PROFILING_DIR, \u001b[33m\"\u001b[39m\u001b[33mfull_training\u001b[39m\u001b[33m\"\u001b[39m)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/tunix/sft/peft_trainer.py:138\u001b[39m, in \u001b[36mPeftTrainer.__init__\u001b[39m\u001b[34m(self, model, optimizer, training_config)\u001b[39m\n\u001b[32m    136\u001b[39m   \u001b[38;5;28mself\u001b[39m.optimizer = nnx.Optimizer(\u001b[38;5;28mself\u001b[39m.model, optimizer, wrt=nnx.LoRAParam)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m   \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_fn = _default_loss_fn\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_loss_fn = _default_loss_fn\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/object.py:143\u001b[39m, in \u001b[36mObjectMeta.__call__\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/object.py:152\u001b[39m, in \u001b[36m_graph_node_meta_call\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m node = \u001b[38;5;28mcls\u001b[39m.\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwargs)\n\u001b[32m    151\u001b[39m \u001b[38;5;28mvars\u001b[39m(node)[\u001b[33m'\u001b[39m\u001b[33m_object__state\u001b[39m\u001b[33m'\u001b[39m] = ObjectState()\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/object.py:146\u001b[39m, in \u001b[36mObjectMeta._object_meta_construct\u001b[39m\u001b[34m(cls, self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_object_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/training/optimizer.py:206\u001b[39m, in \u001b[36mOptimizer.__init__\u001b[39m\u001b[34m(self, model, tx, wrt)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mself\u001b[39m.model = model\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m.tx = tx\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28mself\u001b[39m.opt_state = _wrap_optimizer_state(tx.init(\u001b[43mnnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrt\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m.wrt = wrt\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/graph.py:2520\u001b[39m, in \u001b[36mstate\u001b[39m\u001b[34m(node, *filters)\u001b[39m\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstate\u001b[39m(\n\u001b[32m   2488\u001b[39m   node,\n\u001b[32m   2489\u001b[39m   *filters: filterlib.Filter,\n\u001b[32m   2490\u001b[39m ) -> tp.Union[GraphState, \u001b[38;5;28mtuple\u001b[39m[GraphState, ...]]:\n\u001b[32m   2491\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Similar to :func:`split` but only returns the :class:`State`'s indicated by the filters.\u001b[39;00m\n\u001b[32m   2492\u001b[39m \n\u001b[32m   2493\u001b[39m \u001b[33;03m  Example usage::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2518\u001b[39m \u001b[33;03m    One or more :class:`State` mappings.\u001b[39;00m\n\u001b[32m   2519\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2520\u001b[39m   _, flat_state = \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2521\u001b[39m   state = flat_state.to_nested_state()\n\u001b[32m   2523\u001b[39m   states: GraphState | \u001b[38;5;28mtuple\u001b[39m[GraphState, ...]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv-py311/lib/python3.11/site-packages/flax/nnx/graph.py:680\u001b[39m, in \u001b[36mflatten\u001b[39m\u001b[34m(node, with_paths, return_variables, ref_index, ref_outer_index)\u001b[39m\n\u001b[32m    676\u001b[39m node_impl = get_node_impl(node)\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    678\u001b[39m   \u001b[38;5;28misinstance\u001b[39m(node, Variable) \u001b[38;5;129;01mor\u001b[39;00m variablelib.is_mutable_array(node)\n\u001b[32m    679\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(node)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, this is a bug.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    681\u001b[39m _graph_flatten(\n\u001b[32m    682\u001b[39m   node,\n\u001b[32m    683\u001b[39m   node_impl,\n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m   return_variables,\n\u001b[32m    692\u001b[39m )\n\u001b[32m    693\u001b[39m graphdef: GraphDef = GraphDef(\n\u001b[32m    694\u001b[39m   nodes=nodes, attributes=attributes, num_leaves=\u001b[38;5;28mlen\u001b[39m(leaves)\n\u001b[32m    695\u001b[39m )\n",
            "\u001b[31mRuntimeError\u001b[39m: Unsupported type: <class 'MaxText.layers.models.Transformer'>, this is a bug."
          ]
        }
      ],
      "source": [
        "logging_option = metrics_logger.MetricsLoggerOptions(\n",
        "    log_dir=\"/tmp/tensorboard/full\", flush_every_n_steps=20\n",
        ")\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    metrics_logging_options=logging_option,\n",
        ")\n",
        "trainer = peft_trainer.PeftTrainer(gemma_maxtext_nnx, optax.adamw(1e-5), training_config)\n",
        "trainer = trainer.with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"full_training\")):\n",
        "  with mesh:\n",
        "    trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gd-66SRT3Uv"
      },
      "source": [
        "### Training with LoRA/QLoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZVp0SYMT3Uv"
      },
      "outputs": [],
      "source": [
        "# Restart Colab runtime.\n",
        "\n",
        "training_config = peft_trainer.TrainingConfig(\n",
        "    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    checkpoint_root_directory=CKPT_DIR,\n",
        ")\n",
        "lora_trainer = peft_trainer.PeftTrainer(\n",
        "    lora_gemma, optax.adamw(1e-3), training_config\n",
        ").with_gen_model_input_fn(gen_model_input_fn)\n",
        "\n",
        "with jax.profiler.trace(os.path.join(PROFILING_DIR, \"peft\")):\n",
        "  with mesh:\n",
        "    lora_trainer.train(train_ds, validation_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXUzmyVIT3Uv"
      },
      "source": [
        "### Compare profile results of different training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw2rIKmT3Uv"
      },
      "source": [
        "<font size=3>Setup<font>           | <font size=3>Train Step Time<font> | <font size=3>Peak Memory Usage<font>\n",
        "---------------------------------- | ---------------------------------- | ------------------------------\n",
        "<font size=3>Full weights<font>        |   <font size=3>~1.22 s<font>     |   <font size=3>43.26 GiB<font>\n",
        "<font size=3>QLoRA<font>        |   <font size=3>~1.19 s<font>     |   <font size=3>28.14 GiB<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICLDHTkT3Uv"
      },
      "source": [
        "## Generate with the LoRA/QLoRA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTsmUCK6T3Uv"
      },
      "source": [
        "The QLoRA model still cannot do English-to-French translation properly since we\n",
        "only trained for 100 steps. If you train it for longer, you will see better\n",
        "results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WohZCuY0T3Uw"
      },
      "outputs": [],
      "source": [
        "gemma_tokenizer = data_lib.GemmaTokenizer(\n",
        "    os.path.join(kaggle_ckpt_path, \"tokenizer.model\")\n",
        ")\n",
        "\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=lora_gemma,\n",
        "    tokenizer=gemma_tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=256,\n",
        "        num_layers=model_config.num_layers,\n",
        "        num_kv_heads=model_config.num_kv_heads,\n",
        "        head_dim=model_config.head_dim,\n",
        "    ),\n",
        ")\n",
        "\n",
        "input_batch = [\n",
        "    \"Translate this into French:\\nHello, my name is Morgane.\\n\",\n",
        "    \"Translate this into French:\\nThis dish is delicious!\\n\",\n",
        "    \"Translate this into French:\\nI am a student.\\n\",\n",
        "    \"Translate this into French:\\nHow's the weather today?\\n\",\n",
        "]\n",
        "\n",
        "out_data = sampler(\n",
        "    input_strings=input_batch,\n",
        "    total_generation_steps=10,  # The number of steps performed when generating a response.\n",
        ")\n",
        "\n",
        "for input_string, out_string in zip(input_batch, out_data.text):\n",
        "  print(f\"----------------------\")\n",
        "  print(f\"Prompt:\\n{input_string}\")\n",
        "  print(f\"Output:\\n{out_string}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/tunix/google/examples/qlora_gemma:qlora_demo_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/tunix/google/examples/qlora_gemma/qlora_demo.ipynb",
          "timestamp": 1745566103252
        }
      ]
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
