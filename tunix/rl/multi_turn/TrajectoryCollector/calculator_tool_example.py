import asyncio
from tunix.rl.multi_turn.agents.tool_agent import ToolAgent
from tunix.rl.multi_turn.environments.tool_environment import ToolEnvironment
from tunix.rl.multi_turn.TrajectoryCollector.trajectory_collect_engine import TrajectoryCollectEngine
from tunix.rl.multi_turn.tools.calculator_tool import CalculatorTool
from tunix.rl.multi_turn.rewards.reward import is_two_reward

from openai import OpenAI
from typing import List, Dict

client = OpenAI(api_key="your api key for test, here I'm using gpt to mock qwen")


def openai_model_call(messages: List[Dict[str, str]]) -> str:
    """
    Use OpenAI API to perform model inference and return plain text output
    (may include <tool_call>).

    Args:
        messages: Simulated chat history, e.g., [{'role': 'system', 'content': '...'}, {'role': 'user', 'content': '...'}]

    Returns:
        Plain string response generated by the model (could be <tool_call>{...}</tool_call>)
    """
    response = client.chat.completions.create(
        model="gpt-4.1-nano-2025-04-14",  # Replace with the model you want to use
        messages=messages,
        temperature=0.0,
    )
    return response.choices[0].message.content

def openai_model_call2(messages: List[Dict[str, str]]) -> str:
    """
    Use OpenAI API (completion-style) for inference, returns plain text output.

    Args:
        messages: Simulated chat history.

    Returns:
        Model's plain text response.
    """
    prompt1 = messages_to_prompt(messages)
    print(prompt1)
    response = client.completions.create(
        model="gpt-4.1-nano-2025-04-14",
        prompt=prompt1,
        temperature=0.0,
    )
    return response.choices[0].text

def messages_to_prompt(messages: list[dict]) -> str:
    role_prefix = {
        "system": "[System]",
        "user": "User: ",
        "assistant": "Assistant: ",
        "tool": "[Tool]"
    }
    prompt_lines = []
    for msg in messages:
        prefix = role_prefix.get(msg["role"], "")
        content = msg["content"].strip()
        prompt_lines.append(f"{prefix}{content}")
    return "\n".join(prompt_lines)

# Dummy model inference function
def dummy_model_call(messages):
    return """<tool_call>
{
  "name": "calculator",
  "arguments": {
    "a": 1,
    "b": 1,
    "op": "+"
  }
}
</tool_call>"""

# Dummy final reward calculator
def dummy_final_reward(task, model_response):
    return 1.0  # constant reward

# Main testing coroutine
async def main():
    # Initialize Agent & Environment
    agent = ToolAgent(
        system_prompt="You are Qwen tool assistant. You must use the tools below to solve problems.",
        parser_name="qwen",
        tool_map={"calculator": CalculatorTool}
    )

    env = ToolEnvironment(
        task={"question": "1+1=?"},
        tool_map={"calculator": CalculatorTool},
        reward_fn=is_two_reward,
        max_steps=3
    )

    engine = TrajectoryCollectEngine(
        agent=agent,
        env=env,
        model_call=openai_model_call,
        final_reward_fn=dummy_final_reward,
        max_steps=3,
        gamma=1.0,
        timeout=10.0,
    )

    # Start rollout
    traj = await engine.collect()

    # Output the trajectory
    from pprint import pprint
    print('**********************************************************************************************************************************')
    pprint(traj.to_dict())

# Entry point
if __name__ == "__main__":
    asyncio.run(main())
