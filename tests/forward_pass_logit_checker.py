# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Check if a Tunix supported model's logits match the HF logits."""
# python -m tests.forward_pass_logit_checker --model_name=gemma2-7b-it

import argparse
import os
import numpy as np
import jax
import torch.nn.functional as F
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tunix.models.gemma3 import params as gemma3_params_lib
from tunix.models.gemma3 import model as gemma3_model_lib
from tunix.generate import sampler as sampler_lib
from jax.experimental import multihost_utils


def load_gemma3_model(mesh):
  config = (gemma3_model_lib.Gemma3Config.gemma3_27b()
           )  # pick corresponding config based on model version
  return config, gemma3_params_lib.create_model_from_checkpoint(
      gemma3_params_lib.GEMMA3_27B_IT, config, mesh)


SUPPORTED_MODELS = {
    "gemma2-7b-it": (
        "google/gemma-3-27b-it",
        load_gemma3_model,
        gemma3_params_lib.create_tokenizer,
    )
}


def get_top_k_tokens_scores(logits_tensor,
                            tokenizer_instance,
                            k=10,
                            description=""):
  """Get the top-k tokens and their scores from a given logits tensor."""
  print(f"\n--- {description} top {k} tokens ---")
  collected_tokens = []
  tokens = []
  topk_results = torch.topk(logits_tensor, k=k)
  for i in range(k):
    tok_id = topk_results.indices[i].item()
    score = topk_results.values[i].item()
    tok = tokenizer_instance.decode(tok_id)
    collected_tokens.append({
        "id": int(tok_id),
        "token": tok.strip(),
        "score": float(score)
    })
    tokens.append({
        "id": int(tok_id),
        "token": tok.strip(),
        "score": float(score)
    })

  # Prepare data for logging
  table_str = f"| {'Token ID':<10} | {'Token':<20} | {'Score':<10} |\n"
  table_str += f"|{'-'*12}|{'-'*22}|{'-'*12}|\n"
  for d in collected_tokens:
    table_str += f"| {d['id']:<10} | {d['token']:<20} | {d['score']:<10.4f} |\n"
  print(table_str)
  return tokens


def compare_top_tokens(converted_tokens, golden_tokens):
  """
    Compares two lists of top tokens and calculates similarity metrics.

    Args:
        converted_tokens: top tokens from the converted model.
        golden_tokens:  top tokens from the golden model.
    """
  # Extract the sets of token IDs for comparison
  converted_ids = {token["id"] for token in converted_tokens}
  golden_ids = {token["id"] for token in golden_tokens}

  # --- Metric 1: Overlap Count & Jaccard Similarity ---
  intersection = converted_ids.intersection(golden_ids)
  union = converted_ids.union(golden_ids)

  overlap_count = len(intersection)
  jaccard_similarity = overlap_count / len(union) if union else 0.0

  # --- Metric 2: Rank Agreement ---
  rank_matches = 0
  min_len = min(len(converted_tokens), len(golden_tokens))
  for i in range(min_len):
    if converted_tokens[i]["id"] == golden_tokens[i]["id"]:
      rank_matches += 1

  rank_agreement = (rank_matches / min_len) * 100 if min_len > 0 else 0.0

  metrics = {
      "overlap_count": f"{overlap_count}/{min_len}",
      "jaccard_similarity": jaccard_similarity,
      "rank_agreement_percentage": rank_agreement,
  }

  print("\n--- Similarity Metrics of Top Tokens ---")
  table_str = f"| {'Metric':<30} | {'Value':<20} |\n"
  table_str += f"|{'-'*32}|{'-'*22}|\n"
  for key, value in metrics.items():
    table_str += f"| {key:<30} | {str(value):<20} |\n"
  print(table_str)


def check_kl_divergence(model_logits, golden_logits, atol=0.02):
  """
    Calculates KL divergence D_KL(P_golden || Q_model) over a batch of sequences.

    Args:
        model_logits: Logits from the converted model (Batch, SeqLen, VocabSize).
        golden_logits: Logits from the golden model (Batch, SeqLen, VocabSize).
        token_size: The number of vocabulary entries to consider for the comparison.
                    (Effectively vocab_size_to_compare).
    """
  # 1. Select the relevant vocabulary slice from the logits.
  token_size = min(model_logits.shape[1], golden_logits.shape[1])
  model_logits_sliced = model_logits[..., :token_size]
  golden_logits_sliced = golden_logits[..., :token_size]

  # 2. Reshape
  s, v = model_logits_sliced.shape
  model_logits_reshaped = model_logits_sliced.view(s, v)
  golden_logits_reshaped = golden_logits_sliced.view(s, v)

  # 3. Get the probability distributions.
  golden_probabilities = F.softmax(golden_logits_reshaped, dim=-1)
  model_log_probabilities = F.log_softmax(model_logits_reshaped, dim=-1)

  # 4. Calculate avg KL divergence for all token distributions.
  # use 'batchmean'; the sum of the KL divergences for each token in the batch
  # and then divides by the number of tokens (s)
  kl_div_value = F.kl_div(
      input=model_log_probabilities,
      target=golden_probabilities,
      reduction="batchmean",  # Use 'batchmean' for the average KL per token.
      log_target=False,
  )

  print("\nAverage KL divergence per token (D_KL(P_golden || Q_model)): "
        f"{kl_div_value.item():.6f}")

  # To find the max KL divergence for any single token in the set
  # use reduction='none'.
  kl_divs_per_token = F.kl_div(
      input=model_log_probabilities,
      target=golden_probabilities,
      reduction="none",
      log_target=False,
  ).sum(dim=-1)  # Sum over the vocab dim to get a single KL value per token

  max_kl_div = kl_divs_per_token.max()
  print(f"\nMax KL divergence for a single token in the set: "
        f"{max_kl_div.item():.6f}")

  assert (
      max_kl_div < atol
  ), f"KL divergence values {max_kl_div.item():.6f} exceed the threshold {atol}"


def get_data(golden_data, golden_data_index, config):
  """Get the golden data for the test indexed at golden_data_index"""

  print(f"Comparing forward pass for golden data index = {golden_data_index}")
  print("config.global_batch_size_to_train_on="
        f"{config.global_batch_size_to_train_on}")

  original_ids = np.asarray(golden_data[golden_data_index]["tokens"],
                            dtype=np.int32)
  seq_len = len(original_ids)

  if seq_len > config.max_target_length:
    raise ValueError(f"Golden data sequence length ({seq_len}) is greater than "
                     f"max_target_length ({config.max_target_length})")

  s = (config.global_batch_size_to_train_on, config.max_target_length)

  # Pad ids to max_target_length. Tunix expects 0 for padding.
  padded_ids = np.pad(
      original_ids,
      (0, config.max_target_length - seq_len),
      "constant",
      constant_values=0,
  )
  ids = np.stack(
      [padded_ids for _ in range(config.global_batch_size_to_train_on)])

  logits = np.asarray(golden_data[golden_data_index]["logits"],
                      dtype=np.float32)
  print(f" prompt=\"{golden_data[golden_data_index]['prompt']}\" raw "
        f"ids={original_ids}, logits.shape = {logits.shape}")

  decoder_segment_ids = np.zeros(s, dtype=np.int32)
  decoder_segment_ids[:, :seq_len] = 1
  decoder_positions = np.stack([
      np.arange(config.max_target_length, dtype=np.int32)
      for _ in range(config.global_batch_size_to_train_on)
  ])

  print(f"ids={ids}, decoder_segment_ids = {decoder_segment_ids}, "
        f"decoder_positions= {decoder_positions}")

  return ids, decoder_segment_ids, decoder_positions, logits, seq_len


def convert_jax_weight_to_torch(weight: "jax.Array",
                                dtype: str | None = None) -> torch.Tensor:
  expected_dtype = str(weight.dtype) if dtype is None else dtype
  expected_shape = weight.shape
  weight = multihost_utils.process_allgather(weight)
  weight = np.array(weight, dtype="float32")
  torch_dtype = getattr(torch, expected_dtype)
  torch_array = torch.from_numpy(weight).to(torch_dtype).reshape(expected_shape)
  return torch_array


def main(test_args):
  devices = jax.devices()
  print(f"Running on devices: {devices}")
  mesh = mesh = jax.make_mesh((1, len(devices)), ("fsdp", "tp"))
  hf_name, tunix_model_loader, tunix_tokenizer_loader = SUPPORTED_MODELS[
      test_args.model_name]

  hf_model = AutoModelForCausalLM.from_pretrained(hf_name,
                                                  torch_dtype=torch.bfloat16)
  tokenizer = AutoTokenizer.from_pretrained(hf_name)

  config, tunix_model = tunix_model_loader(mesh)
  tunix_tokenizer = tunix_tokenizer_loader()
  sampler = sampler_lib.Sampler(
      tunix_model,
      tunix_tokenizer,
      sampler_lib.CacheConfig(
          cache_size=test_args.max_target_length + 5,
          num_layers=config.num_layers,
          num_kv_heads=config.num_kv_heads,
          head_dim=config.head_dim,
      ),
  )
  print(f"Tokenizer vocab size: {tokenizer.vocab_size} vs "
        f"{tunix_tokenizer.vocab_size()}")
  prompts = ["I love to", "Today is a", "What is the"]
  for input_text in prompts:
    print(f"\n--- Prompt: {input_text} ---")

    # Tokenize for HF
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,
        max_length=test_args.max_target_length,
        truncation=True,
    )
    actual_seq_len = inputs["input_ids"].shape[1]

    # --- HF Forward Pass ---
    with torch.no_grad():
      hf_logits_torch = hf_model(**inputs).logits[0]

    # --- Tunix Forward Pass ---
    output = sampler(
        [input_text],
        total_generation_steps=1,
        max_prompt_length=test_args.max_target_length,
        echo=True,
        return_logits=True,
    )
    tunix_logits_jax = output.logits[0]
    tunix_logits_jax_sliced = tunix_logits_jax[-actual_seq_len:, :]
    tunix_logits_torch = convert_jax_weight_to_torch(tunix_logits_jax_sliced)

    # --- Compare logits for the last token prediction ---
    hf_last_token_logits = hf_logits_torch[-1, :]
    tunix_last_token_logits = (tunix_logits_torch[-1, :]
                              )  # Tunix output already sliced to actual_seq_len

    tokens_Tunix = get_top_k_tokens_scores(tunix_last_token_logits,
                                           tokenizer,
                                           k=10,
                                           description="Tunix model")
    tokens_hf = get_top_k_tokens_scores(
        hf_last_token_logits, tokenizer, k=10, description="HF model")
    compare_top_tokens(converted_tokens=tokens_Tunix, golden_tokens=tokens_hf)

    # --- Compare all logits in the sequence (for the first batch item) ---
    # Unsqueeze to add batch dimension for check_kl_divergence: [1, seq, vocab]
    check_kl_divergence(tunix_logits_torch, hf_logits_torch, atol=test_args.max_kl_div)


if __name__ == "__main__":
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"  # Suppress TensorFlow logging

  parser = argparse.ArgumentParser()
  parser.add_argument("--model_name",
                      type=str,
                      required=True,
                      help="Name of the model to test.")
  parser.add_argument("--atol", type=float, required=False, default=0.1)
  parser.add_argument("--rtol", type=float, required=False, default=0.1)
  parser.add_argument("--token_size", type=int, required=False)
  parser.add_argument("--max_kl_div", type=float, required=False, default=0.015)
  parser.add_argument("--max_target_length",
                      type=int,
                      required=False,
                      default=1024)
  test_args, _ = parser.parse_known_args()

  assert (
      test_args.model_name in SUPPORTED_MODELS.keys()
  ), f"Model {test_args.model_name} is not supported."

  main(test_args)
